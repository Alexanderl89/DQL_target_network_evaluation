{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from classes_and_functions.plot import plot_graph\n",
    "from classes_and_functions.serialize import serialize_loss_step_reward\n",
    "from classes_and_functions.ini_agent_replay_buffer import initialize_agent_and_replay_buffer\n",
    "from classes_and_functions.train_and_evaluate import train_and_eval\n",
    "\n",
    "config = {  \"EPISODES\": 10000,\n",
    "            \"BATCH_SIZE\": 256,\n",
    "            \"BUFFER_SIZE\": 100000,\n",
    "            \"DISCOUNT_FACTOR\": 0.95,\n",
    "            \"TARGET_UPDATE\": 10,\n",
    "            \"DECAY_TIME\": 100,\n",
    "            \"ep\": 1,\n",
    "            \"INITIAL_EP\": 1,\n",
    "            \"MIN_EP\": 0.01,\n",
    "            \"POWER_EP\": 7,\n",
    "            \"lr\": 0.02,\n",
    "            \"INITIAL_LR\": 0.02,\n",
    "            \"MIN_LR\": 0.0001,\n",
    "            \"POWER_LR\": 2,\n",
    "            \"LAYERS\": [64, 128],\n",
    "            \"ACTIVATION_FUNCTION\": nn.Sigmoid(),\n",
    "            \"SEED\": 24,\n",
    "            \"ENVIRONMENT\": 'CartPole-v1',\n",
    "        }\n",
    "\n",
    "path = f\"comp_online/seed_{config['SEED']}\"\n",
    "\n",
    "folder_is_exists = os.path.exists(path)\n",
    "if not folder_is_exists:\n",
    "    os.makedirs(path)\n",
    "    os.makedirs(f\"{path}/results\")\n",
    "\n",
    "\n",
    "with open(f\"{path}/settings.txt\", \"w\") as f:\n",
    "    for key, value in config.items():\n",
    "        f.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training of the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# INITIALIZE agent and replay buffer\n",
    "dqn_agent,replay_buffer = initialize_agent_and_replay_buffer(config)\n",
    "# Train agent with target network\n",
    "mean_loss_list,step_list,reward_list = train_and_eval(dqn_agent,replay_buffer,config,start_index=1,stop_index=config['EPISODES'],TN=True)\n",
    "\n",
    "# SERIALIZE\n",
    "serialize_loss_step_reward(mean_loss_list,step_list,reward_list,\"\",f\"{path}/results\")\n",
    "\n",
    "# GRAPH 1\n",
    "plot_graph(mean_loss_list,\"Episodes\",\"Mean loss\",color=\"orange\",ylim=[0,10],path_name=f\"{path}/mean_loss\")\n",
    "\n",
    "# GRAPH 2\n",
    "plot_graph(step_list,\"Episodes\",\"Steps\",type=plt.bar,color=\"green\",path_name=f\"{path}/steps\")\n",
    "\n",
    "# GRAPH 3\n",
    "plot_graph(reward_list,\"Episodes\",\"Reward\",type=plt.plot,color=\"blue\",path_name=f\"{path}/reward\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Searching for a stabilization point through training an agent without a target network after some number of episodes***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_cases = [ int(i) for i in range(920,1100,10)]\n",
    "\n",
    "\n",
    "print(test_cases)\n",
    "\n",
    "for n in test_cases:\n",
    "    print(f\"Training model {n}\")\n",
    "    config['ep'] = config['INITIAL_EP']\n",
    "    config['lr'] = config['INITIAL_LR']\n",
    "    \n",
    "    # initialize the training with target network for the first n episodes\n",
    "    dqn_agent,replay_buffer = initialize_agent_and_replay_buffer(config)\n",
    "    mll_TN,sl_TN,rl_TN= train_and_eval(dqn_agent,replay_buffer,config,start_index=1,stop_index=n,TN=True)\n",
    "    \n",
    "    # continue training without target network for the rest of the episodes\n",
    "    print(\"Without target network\")\n",
    "    mll,sl,rl = train_and_eval(dqn_agent,replay_buffer,config,start_index=n+1,stop_index=config['EPISODES'],TN=False)\n",
    "    \n",
    "    \n",
    "    mean_loss_list = mll_TN + mll\n",
    "    step_list = sl_TN + sl\n",
    "    reward_list = rl_TN + rl\n",
    "    \n",
    "    #SERIALIZE\n",
    "    serialize_loss_step_reward(mean_loss_list,step_list,reward_list,f\"_{n}\",f\"{path}/results\")\n",
    "    \n",
    "    # GRAPH 1\n",
    "    plot_graph(mean_loss_list,\"Episodes\",\"Mean loss\",type=plt.plot,color=\"orange\",ylim=[0,5],path_name=f\"{path}/mean_loss_{n}\")\n",
    "\n",
    "    # GRAPH 2\n",
    "    plot_graph(step_list,\"Episodes\",\"Steps\",type=plt.bar,color=\"green\",path_name=f\"{path}/steps_{n}\")\n",
    "\n",
    "    # GRAPH 3\n",
    "    plot_graph(reward_list,\"Episodes\",\"Reward\",type=plt.plot,color=\"blue\",path_name=f\"{path}/reward_{n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from classes_and_functions.train_and_evaluate import train_and_eval, train_agent, evaluate_agent\n",
    "from classes_and_functions.plot import plot_graph\n",
    "from classes_and_functions.serialize import serialize_loss_step_reward\n",
    "from classes_and_functions.ini_agent_replay_buffer import initialize_agent_and_replay_buffer\n",
    "from classes_and_functions.serialize import serialize_replay_buffer,deserialize_replay_buffer\n",
    "from classes_and_functions.polynomial_decay import polynomial_decay\n",
    "\n",
    "config = {  \"DISCOUNT_FACTOR\": 0.95,\n",
    "            \"TARGET_UPDATE\": 10,\n",
    "            \"DECAY_TIME\": 100,\n",
    "            \"MIN_EP\": 0.01,\n",
    "            \"POWER_EP\": 7,\n",
    "            \"MIN_LR\": 0.0001,\n",
    "            \"POWER_LR\": 2,\n",
    "            \"LAYERS\": [64, 128],\n",
    "            \"ACTIVATION_FUNCTION\": \"Sigmoid\",\n",
    "            \"SEED\": 24\n",
    "        }\n",
    "\n",
    "config_expert_agent = {  \"EPISODES\": 10000,\n",
    "            \"BATCH_SIZE\": 256,\n",
    "            \"BUFFER_SIZE\": 500000,\n",
    "            \"ep\": 1,\n",
    "            \"INITIAL_EP\": 1,\n",
    "            \"lr\": 0.02,\n",
    "            \"INITIAL_LR\": 0.02,\n",
    "        }\n",
    "\n",
    "config_pre_training = {  \"REPLAYS\": 200000,\n",
    "            \"BATCH_SIZE\": 32,\n",
    "            \"BUFFER_SIZE\": 500000,\n",
    "            \"ep\": 1,\n",
    "            \"INITIAL_EP\": 1,\n",
    "            \"lr\": 0.02,\n",
    "            \"INITIAL_LR\": 0.02,\n",
    "        }\n",
    "\n",
    "config_online_training = {  \"EPISODES\": 10000,\n",
    "            \"BATCH_SIZE\": 256,\n",
    "            \"BUFFER_SIZE\": 100000,\n",
    "            \"ep\": 1,\n",
    "            \"INITIAL_EP\": 1,\n",
    "            \"lr\": 0.005,\n",
    "            \"INITIAL_LR\": 0.005,\n",
    "        }\n",
    "\n",
    "\n",
    "path = f\"comp_offline/seed_{config['SEED']}\"\n",
    "\n",
    "folder_is_exists = os.path.exists(path)\n",
    "if not folder_is_exists:\n",
    "    os.makedirs(path)\n",
    "    os.makedirs(f\"{path}/results\")\n",
    "\n",
    "with open(f\"{path}/settings.txt\", \"w\") as f:\n",
    "    f.write(\"config\\n\")\n",
    "    f.write(str(config))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"config_expert_agent\\n\")\n",
    "    f.write(str(config_expert_agent))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"config_pre_training\\n\")\n",
    "    f.write(str(config_pre_training))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"config_online_training\\n\")\n",
    "    f.write(str(config_online_training))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "config_expert_agent.update(config)\n",
    "config_pre_training.update(config)\n",
    "config_online_training.update(config)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Train an agent in order to achieve a replay buffer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_learning_buffer(dqn_agent,replay_buffer,config):\n",
    "    \n",
    "    \n",
    "    mean_loss_list = []\n",
    "    step_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "    for i in range(1,config['EPISODES']+1):\n",
    "        dqn_agent.train_mode()\n",
    "        mean_loss = train_agent(dqn_agent,replay_buffer,ep=config['ep'] ,batch_size=config['BATCH_SIZE'],TN=True,seed=i)\n",
    "        dqn_agent.evaluate_mode()\n",
    "        steps,reward = evaluate_agent(dqn_agent,graphical=False,seed=i)\n",
    "\n",
    "        mean_loss_list.append(mean_loss)\n",
    "        step_list.append(steps)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "\n",
    "        if i % config['DECAY_TIME'] == 0:\n",
    "            config['ep'] = polynomial_decay(config['INITIAL_EP'],config['MIN_EP'],i,config['EPISODES'],config['POWER_EP'])\n",
    "\n",
    "        if i % config['DECAY_TIME'] == 0 and len(reward_list) > 100 and np.mean(reward_list[-100:]) > 2500:\n",
    "            config['lr'] = polynomial_decay(config['INITIAL_LR'],config['MIN_LR'],i,config['EPISODES'],config['POWER_LR'])\n",
    "            dqn_agent.set_learning_rate(config['lr'])\n",
    "\n",
    "\n",
    "        if i % config['TARGET_UPDATE'] == 0:\n",
    "            dqn_agent.update_target_q_network()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Episodes: {i} eb: {round(config['ep'],5)}, lr: {round(dqn_agent.get_learning_rate(),5)}, mean loss: {round(np.mean(mean_loss_list[-100:]),5)} ,Steps: {round(np.mean(step_list[-100:]),5)}, Reward: {round(np.mean(reward_list[-100:]),0)} , buffer size: {replay_buffer.buffer_size()}\")\n",
    "            \n",
    "\n",
    "        if replay_buffer.buffer_size() >= config['BUFFER_SIZE']:\n",
    "            serialize_replay_buffer(replay_buffer,path)\n",
    "            break\n",
    "\n",
    "    return mean_loss_list,step_list,reward_list\n",
    "\n",
    "\n",
    "# INITIALIZE agent and replay buffer\n",
    "agent_TN,replay_buffer_TN = initialize_agent_and_replay_buffer(config=config_expert_agent)\n",
    "# Train agent with target network\n",
    "mean_loss_list,step_list,reward_list = get_expert_learning_buffer(agent_TN,replay_buffer_TN,config_expert_agent)\n",
    "\n",
    "# SERIALIZE\n",
    "serialize_loss_step_reward(mean_loss_list,step_list,reward_list,\"PRE_TN\",f\"{path}/results\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize an agent. Pre train in offline mode and continue in online mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replays_l = [400000,450000,500000,550000,600000,650000,700000,750000]\n",
    "\n",
    "def pre_training(agent,replay_buffer,replays,config,TN=False):\n",
    "    mean_loss_list = []\n",
    "    for i in range(1,replays+1):\n",
    "        agent.train_mode()\n",
    "        loss = agent.replay(replay_buffer,batch_size=config['BATCH_SIZE'],target_network=TN)\n",
    "        mean_loss_list.append(loss)\n",
    "        \n",
    "        if i % 1000 == 0:        \n",
    "            agent.evaluate_mode()\n",
    "            steps,reward = evaluate_agent(agent,config,graphical=False)\n",
    "            print(f\"[{i}] mean loss: {round(np.mean(mean_loss_list[-100:]),5)}, steps: {int(steps)}, reward: {int(reward)}\")\n",
    "            \n",
    "    return mean_loss_list\n",
    "\n",
    "\n",
    "replay_buffer_PRE = deserialize_replay_buffer(path)\n",
    "\n",
    "for rep in replays_l:\n",
    "    dqn_agent_PRE,_ = initialize_agent_and_replay_buffer(config=config_pre_training)\n",
    "    mean_loss = pre_training(dqn_agent_PRE,replay_buffer_PRE,rep,config_pre_training,TN=False)\n",
    "    serialize_loss_step_reward(mean_loss,[],[],f\"PRE_{rep}\",f\"{path}/results\")\n",
    "\n",
    "    config_online_training['ep'] = config_online_training['INITIAL_EP']\n",
    "    config_online_training['lr'] = config_online_training['INITIAL_LR']\n",
    "\n",
    "    dqn_agent_PRE.set_learning_rate(config_online_training['INITIAL_LR'])\n",
    "    _,replay_buffer = initialize_agent_and_replay_buffer(config=config_online_training)\n",
    "    mean_loss_list,step_list,reward_list = train_and_eval(dqn_agent_PRE,replay_buffer,config_online_training,stop_index=config_online_training['EPISODES'],TN=False)\n",
    "\n",
    "    serialize_loss_step_reward(mean_loss_list,step_list,reward_list,f\"_{rep}\",f\"{path}/results\")\n",
    "\n",
    "    # GRAPH 1\n",
    "    plot_graph(mean_loss_list,\"Episodes\",\"Mean loss\",color=\"orange\",ylim=[0,10],path_name=f\"{path}/mean_loss_{rep}\")\n",
    "\n",
    "    # GRAPH 2\n",
    "    plot_graph(step_list,\"Episodes\",\"Steps\",type=plt.bar,color=\"green\",path_name=f\"{path}/steps_{rep}\")\n",
    "\n",
    "    # GRAPH 3\n",
    "    plot_graph(reward_list,\"Episodes\",\"Reward\",type=plt.plot,color=\"blue\",path_name=f\"{path}/reward_{rep}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
